# Data Pipeline with Airflow

## Summary
- Built a dynamic and reusable ETL pipeline through data quality checks
- All the tasks have a dependency and DAG begins with a start_execution task and ends with a end_execution task.

## Airflow Connections to AWS
![](https://raw.githubusercontent.com/gyhou/millionsongs/master/img/admin-connections.png)

1. Use Airflow's UI to configure AWS credentials and connection to Redshift
1. At the top **Admin** tab and select **Connections**
1. Under **Connections**, select **Create**
1. On the create connection page, enter the following values:
    - **Conn Id**: Enter `aws_credentials`
    - **Conn Type**: Enter `Amazon Web Services`
    - **Login**: Enter your **Access key ID** from the IAM User credentials
    - **Password**: Enter your **Secret access key** from the IAM User credentials
    - Once you've entered these values, select **Save and Add Another**
1. On the next create connection page, enter the following values:
    - **Conn Id**: Enter `redshift`
    - **Conn Type**: Enter `Postgres`
    - **Host**: Enter the endpoint of your Redshift cluster, excluding the port at the end. You can find this by selecting your cluster in the **Clusters** page of the Amazon Redshift console. See where this is located in the screenshot below
      - ***IMPORTANT***: Make sure to **NOT** include the port at the end of the Redshift endpoint string
    - **Schema**: Enter the Redshift `Database Name` to connect to
    - **Login**: Enter the Redshift `Master Username` to connect to
    - **Password**: Enter the password you created when launching your Redshift cluster.
    - **Port**: Enter `5439`
    - Once you've entered these values, select **Save**

**WARNING: Remember to DELETE your cluster each time you are finished working to avoid large, unexpected costs.**

![](https://raw.githubusercontent.com/gyhou/millionsongs/master/img/cluster-details.png)

## Project Datasets
### Song Dataset
**S3 Song Data Source** - `s3://udacity-dend/song_data`
- Subset of the real data from [Million Song Dataset](http://millionsongdataset.com/)
- Each file is in JSON format and contains metadata about a song and the artist of that song

### Log Dataset
**S3 Log Data Source** - `s3://udacity-dend/log_data`
- Log files are in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above
- These simulate activity logs from a music streaming app based on specified configurations
- The dataset are partitioned by year and month

## Dag configuration
![](https://raw.githubusercontent.com/gyhou/millionsongs/master/img/songplay-dag.png)

### DAG contains default_args dict, with the following keys:
- Owner
- Depends_on_past
- Start_date
- Retries
- Retry_delay
- Catchup
- DAG can be scheduled

## Database schema design for Song Play Analysis
![](https://raw.githubusercontent.com/gyhou/millionsongs/master/img/Song_ERD.png)

## Staging the data
- Stages data from S3 to Redshift
  - Data is loaded to the staging tables in Redshift using `COPY` statement
- Uses params to generate the copy statement dynamically
  - Dynamically generated copy statements as opposed to static SQL statements
- Operator contains logging in different steps of the execution
  - `logging.info` shows the progress of staging load
- SQL statements are executed using a Airflow hook
  - Connected to the Redshift database via an Airflow hook

## Loading dimensions and fact table
- Dimensions are loaded with on the `LoadDimension` operator
 - Separated functional operator for dimensions (`LoadDimensionOperator`)
- Facts are loaded with on the `LoadFact` operator
 - Separated functional operator for facts (`LoadFactOperator`)
- Task uses params to generate the copy statement dynamically
  - Dynamically generated copy statements as opposed to static SQL statements
- DAG allows to switch between append-only and delete-load functionality

## Data Quality Checks
- Data quality check is done with correct operator
- Set up DAG to either fail or retries 3 times
- Operator uses params to get the tests and the results, tests are not hard coded to the operator
